{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670c06d0",
   "metadata": {},
   "source": [
    "4.2\\. Find the outer product of the following two vecotrs (10 MINUTES)\n",
    "\n",
    "```python\n",
    "u = np.array([1,3,5,7])\n",
    "v = np.array([2,4,6,8])\n",
    "```\n",
    "\n",
    "Do this in the following ways:\n",
    "\n",
    "   * Using the function outer in numpy\n",
    "   * Using a nested for loop or list comprehension\n",
    "   * Using numpy broadcasting operatoins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b06049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e24bf60",
   "metadata": {},
   "source": [
    "4.4\\. Use np.linspace to create an array of 100 numbers between 0 and 2π (inclusive). (10 MINUTES)\n",
    "\n",
    "  * Extract every 10th element using slice notation\n",
    "  * Reverse the array using slice notation\n",
    "  * Extract elements where the absolute difference between the sine and cosine functions evaluated at that element is less than 0.1\n",
    "  * Make a plot showing the sin and cos functions and indicate where they are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23373331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "141da825",
   "metadata": {},
   "source": [
    "4.6\\. Use broadcasting to create a grid of distances (15 MINUTES)\n",
    "\n",
    "Route 66 crosses the following cities in the US: Chicago, Springfield, Saint-Louis, Tulsa, Oklahoma City, Amarillo, Santa Fe, Albuquerque, Flagstaff, Los Angeles\n",
    "The corresponding positions in miles are: 0, 198, 303, 736, 871, 1175, 1475, 1544, 1913, 2448\n",
    "\n",
    "  * Construct a 2D grid of distances among each city along Route 66\n",
    "  * Convert that in km (those savages...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6e9721",
   "metadata": {},
   "source": [
    "4.7\\. Prime numbers sieve: compute the prime numbers in the 0-N (N=99 to start with) range with a sieve (mask). (20 MINUTES)\n",
    "  * Constract a shape (100,) boolean array, the mask\n",
    "  * Identify the multiples of each number starting from 2 and set accordingly the corresponding mask element\n",
    "  * Apply the mask to obtain an array of ordered prime numbers\n",
    "  * Check the performances (timeit); how does it scale with N?\n",
    "  * Implement the optimization suggested in the [sieve of Eratosthenes](https://en.wikipedia.org/wiki/Sieve_of_Eratosthenes)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2605f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cbe7d6d",
   "metadata": {},
   "source": [
    "4.8\\. Diffusion using random walk (30 MINUTES)\n",
    "\n",
    "Consider a simple random walk process: at each step in time, a walker jumps right or left (+1 or -1) with equal probability. The goal is to find the typical distance from the origin of a random walker after a given amount of time. \n",
    "To do that, let's simulate many walkers and create a 2D array with each walker as a raw and the actual time evolution as columns\n",
    "\n",
    "  * Take 1000 walkers and let them walk for 200 steps\n",
    "  * Use randint to create a 2D array of size walkers x steps with values -1 or 1\n",
    "  * Build the actual walking distances for each walker (i.e. another 2D array \"summing on each raw\")\n",
    "  * Take the square of that 2D array (elementwise)\n",
    "  * Compute the mean of the squared distances at each step (i.e. the mean along the columns)\n",
    "  * Plot the average distances (sqrt(distance\\*\\*2)) as a function of time (step)\n",
    "  \n",
    "Did you get what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43aa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cce99ac9",
   "metadata": {},
   "source": [
    "4.9\\. Analyze a data file (15 MINUTES)\n",
    "  * Download the population of hares, lynxes and carrots at the beginning of the last century.\n",
    "    ```python\n",
    "    ! wget https://www.dropbox.com/s/3vigxoqayo389uc/populations.txt\n",
    "    ```\n",
    "\n",
    "  * Check the content by looking within the file\n",
    "  * Load the data (use an appropriate numpy method) into a 2D array\n",
    "  * Create arrays out of the columns, the arrays being (in order): *year*, *hares*, *lynxes*, *carrots* \n",
    "  * Plot the 3 populations over the years\n",
    "  * Compute the main statistical properties of the dataset (mean, std, correlations, etc.)\n",
    "  * Which species has the highest population each year?\n",
    "\n",
    "Do you feel there is some evident correlation here? [Studies](https://www.enr.gov.nt.ca/en/services/lynx/lynx-snowshoe-hare-cycle) tend to believe so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeef43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c35d5d9",
   "metadata": {},
   "source": [
    "5.4\\. load the binary file named *credit_card.dat* and convert the data into the real credit-card number (20 MINUTES).\n",
    "\n",
    "Each line correspond to a credit card number.\n",
    "\n",
    "Each character is composed by 6 bit (even the space) and the last 4 bit are just a padding\n",
    "\n",
    "**hint**: use the `chr()` function to convert a number to a char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26b6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d572d89",
   "metadata": {},
   "source": [
    "## Pandas analysis (60 MINUTES)\n",
    "\n",
    "This exercise consists in analyzing a dataset containg timing information from a series of Time-to-Digital-Converters (TDC) implemented in a couple of FPGAs. Each measurement (i.e. each row of the input file) consists of a flag that specifies the type of message ('HEAD', which in this case is always 1), two addresses of the TDC providing the signal ('FPGA' and 'TDC_CHANNEL'), and the timing information ('ORBIT_CNT', 'BX_COUNTER', and 'TDC_MEAS'). Each TDC count corresponds to 25/30 ns, whereas a unit of BX_COUNTER corresponds to 25 ns, and the ORBIT_CNT is increased every 'x' BX_COUNTER. This allows to store the time in a similar way to hours, minutes and seconds.\n",
    "\n",
    "1\\. Create a Pandas DataFrame reading N rows of the 'data_000637.txt' dataset. Choose N to be smaller than or equal to the maximum number of rows and larger that 10k.\n",
    "\n",
    "2\\. Find out the number of BX in a ORBIT (the value 'x').\n",
    "\n",
    "3\\. Find out how much the data taking lasted. You can either make an estimate based on the fraction of the measurements (rows) you read, or perform this check precisely by reading out the whole dataset.\n",
    "\n",
    "4\\. Create a new column with the absolute time in ns (as a combination of the other three columns with timing information).\n",
    "\n",
    "5\\. Replace the values (all 1) of the HEAD column randomly with 0 or 1.\n",
    "\n",
    "6\\. Create a new DataFrame that contains only the rows with HEAD=1.\n",
    "\n",
    "7\\. Make two occupancy plots (one for each FPGA), i.e. plot the number of counts per TDC channel\n",
    "\n",
    "8\\. Use the groupby method to find out the noisy channels, i.e. the TDC channels with most counts (say the top 3)\n",
    "\n",
    "9\\. Count the number of unique orbits. Count the number of unique orbits with at least one measurement from TDC_CHANNEL=139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795fe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddba5da8",
   "metadata": {},
   "source": [
    "7.1\\. **Kernel Density Estimate** (40 MINUTES)\n",
    "\n",
    "Produce a KDE for a given distribution (by hand, not using seaborn!):\n",
    "\n",
    "* Fill a numpy array, x,  of len(N) (with N=O(100)) with a variable normally distributed, with a given mean a standard deviation\n",
    "* Fill an histogram in pyplot taking properly care about the aesthetic\n",
    "   * use a meaningful number of bins\n",
    "   * set a proper y axis label\n",
    "   * set proper value of y axis major ticks labels (e.g. you want to display only integer labels)\n",
    "   * display the histograms as data points with errors (the error being the poisson uncertainty)\n",
    "* for every element of x, create a gaussian with the mean corresponding the element value and std as a parameter that can be tuned. The std default value should be:\n",
    "$$ 1.06 * x.std() * x.size ^{-\\frac{1}{5.}} $$\n",
    "you can use the scipy function `stats.norm()` for that.\n",
    "* In a separate plot (to be placed beside the original histogram), plot all the gaussian functions so obtained\n",
    "* Sum (with np.sum()) all the gaussian functions and normalize the result such that the integral matches the integral of the original histogram. For that you could use the `scipy.integrate.trapz()` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb0c50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c832c3ca",
   "metadata": {},
   "source": [
    "7.2\\. **Color-coded scatter plot** (10 MINUTES)\n",
    "\n",
    "Produce a scatter plot out of a dataset with two categories\n",
    "\n",
    "* Write a function that generate a 2D datasets of 2 categories. Each category should distribute as a 2D gaussian with a given mean and std (clearly it is better to have different values means..)\n",
    "* Display the dataset in a scatter plot marking the two categories with different marker colors.\n",
    "\n",
    "An example is given below\n",
    "\n",
    "You can try to make the procedure more general by allowing a given number $n\\ge 2$ of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6762c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78d068bf",
   "metadata": {},
   "source": [
    "7.3\\. **Profile plot** (30 MINUTES)\n",
    "\n",
    "Produce a profile plot from a scatter plot.\n",
    "* Download the following dataset and load it as a pandas dataframe:\n",
    "```bash\n",
    "wget https://www.dropbox.com/s/hgnvyj9abatk8g6/residuals_261.npy\n",
    "```\n",
    "Note that you should use the `np.load()` function to load the file as a numpy array, call the `.item()` method, and then pass it to the `pd.DataFrame()` constructor.\n",
    "* Inspect the dataset, you'll find two variables (features)\n",
    "* Clean the sample by selecting the entries (rows) with the variable \"residual\" in absolute value smaller than 2\n",
    "* perform a linear regression of \"residuals\" versus \"distances\" using `scipy.stats.linregress()` \n",
    "* plot a seaborn jointplot of  \"residuals\" versus \"distances\", having seaborn performing a linear regression. The result of the regression should be displayed on the plot\n",
    "* Fill 3 numpy arrays\n",
    "  * x, serving as an array of bin centers for the \"distance\" variable. It should range from 0 to 20 with reasonable number of steps (bins)\n",
    "  * y, the mean values of the \"residuals\", estimated in slices (bins) of \"distance\"\n",
    "  * erry, the standard deviation of the  of the \"residuals\", estimated in slices (bins) of \"distance\"\n",
    "* Plot the profile plot on top of the scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a66be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cafa8acc",
   "metadata": {},
   "source": [
    "8.1\\. **PCA on 3D dataset** (45 MINUTES)\n",
    "\n",
    "* Generate a dataset with 3 features each with N entries (N being ${\\cal O}(1000)$). With $N(\\mu,\\sigma)$ the normali distribution with mean $\\mu$ and $\\sigma$  standard deviation, generate the 3 variables $x_{1,2,3}$ such that:\n",
    "    * $x_1$ is distributed as $N(0,1)$\n",
    "    * $x_2$ is distributed as $x_1+N(0,3)$\n",
    "    * $x_3$ is given by $2x_1+x_2$\n",
    "* Find the eigenvectors and eigenvalues of the covariance matrix of the dataset\n",
    "* Find the eigenvectors and eigenvalues using SVD. Check that the two procedures yield to same result\n",
    "* What percent of the total dataset's variability is explained by the principal components? Given how the dataset was constructed, do these make sense? Reduce the dimensionality of the system so that at least 99% of the total variability is retained.\n",
    "* Redefine the data in the basis yielded by the PCA procedure\n",
    "* Plot the data points in the original and the new coordiantes as a set of scatter plots. Your final figure should have 2 rows of 3 plots each, where the columns show the (0,1), (0,2) and (1,2) proejctions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fedd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2fc7bbc",
   "metadata": {},
   "source": [
    "8.3 \\. **Looking at an oscillating spring** (60 MINUTES)\n",
    "\n",
    "Imagine you have $n$ cameras looking at a spring oscillating along the $x$ axis. Each  camera record the motion of the spring looking at it along a given direction defined by the pair $(\\theta_i, \\phi_i)$, the angles in spherical coordinates. \n",
    "\n",
    "Start from the simulation of the records (say ${\\cal O}(1000)$) of the spring's motion along the x axis, assuming a little random noise affects the measurements along the $y$. Rotate such dataset to emulate the records of each camera.\n",
    "\n",
    "Perform a Principal Component Analysis on the thus obtained dataset, aiming at finding the only one coordinate that really matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce32c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00cfe7a7",
   "metadata": {},
   "source": [
    "9.1\\. **Maximum wind speed prediction at the Sprogø station** (30 MINUTES)\n",
    "\n",
    "The exercise goal is to predict the maximum wind speed occurring every 50 years even if no measure exists for such a period. The available data are only measured over 21 years at the Sprogø meteorological station located in Denmark. \n",
    "\n",
    "The annual maxima are supposed to fit a normal probability density function. However such function is not going to be estimated because it gives a probability from a wind speed maxima. Finding the maximum wind speed occurring every 50 years requires the opposite approach, the result needs to be found from a defined probability. That is the quantile function role and the exercise goal will be to find it. In the current model, it is supposed that the maximum wind speed occurring every 50 years is defined as the upper 2% quantile.\n",
    "\n",
    "By definition, the quantile function is the inverse of the cumulative distribution function. The latter describes the probability distribution of an annual maxima. In the exercise, the cumulative probability $p_i$ for a given year i is defined as $p_i = i/(N+1)$ with $N = 21$, the number of measured years. Thus it will be possible to calculate the cumulative probability of every measured wind speed maxima. From those experimental points, the scipy.interpolate module will be very useful for fitting the quantile function. Finally the 50 years maxima is going to be evaluated from the cumulative probability of the 2% quantile.\n",
    "\n",
    "Practically, load the dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "max_speeds = np.load('max-speeds.npy')\n",
    "years_nb = max_speeds.shape[0]\n",
    "```\n",
    "\n",
    "Compute then the cumulative probability $p_i$ (`cprob`) and sort the maximum speeds from the data. Use then the  UnivariateSpline from scipy.interpolate to define a quantile function and thus estimate the probabilities.\n",
    "\n",
    "In the current model, the maximum wind speed occurring every 50 years is defined as the upper 2% quantile. As a result, the cumulative probability value will be:\n",
    "\n",
    "```python\n",
    "fifty_prob = 1. - 0.02\n",
    "```\n",
    "\n",
    "So the storm wind speed occurring every 50 years can be guessed as:\n",
    "\n",
    "``` python\n",
    "fifty_wind = quantile_func(fifty_prob)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f804c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4d4e406",
   "metadata": {},
   "source": [
    "9.2\\. **Curve fitting of temperature in Alaska** (20 MINUTES)\n",
    "\n",
    "The temperature extremes in Alaska for each month, starting in January, are given by (in degrees Celcius):\n",
    "\n",
    "max:  17,  19,  21,  28,  33,  38, 37,  37,  31,  23,  19,  18\n",
    "\n",
    "min: -62, -59, -56, -46, -32, -18, -9, -13, -25, -46, -52, -58\n",
    "\n",
    "* Plot these temperature extremes.\n",
    "* Define a function that can describe min and max temperatures. \n",
    "* Fit this function to the data with scipy.optimize.curve_fit().\n",
    "* Plot the result. Is the fit reasonable? If not, why?\n",
    "* Is the time offset for min and max temperatures the same within the fit accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb3003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02e83be2",
   "metadata": {},
   "source": [
    "9.3\\. **2D minimization of a six-hump camelback function** (20 MINUTES)\n",
    "\n",
    "$$\n",
    "f(x,y) = \\left(4-2.1x^2+\\frac{x^4}{3} \\right) x^2 +xy + (4y^2 -4)y^2\n",
    "$$\n",
    "\n",
    "has multiple global and local minima. Find the global minima of this function.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Variables can be restricted to $-2 < x < 2$ and $-1 < y < 1$.\n",
    "* Use numpy.meshgrid() and pylab.imshow() to find visually the regions.\n",
    "* Use scipy.optimize.minimize(), optionally trying out several of its methods.\n",
    "\n",
    "How many global minima are there, and what is the function value at those points? What happens for an initial guess of $(x, y) = (0, 0)$ ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e6a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2f1c628",
   "metadata": {},
   "source": [
    "9.5\\. **FFT of an image** (30 MINUTES)\n",
    "\n",
    "* Examine the provided image `moonlanding.png`, which is heavily contaminated with periodic noise. In this exercise, we aim to clean up the noise using the Fast Fourier Transform.\n",
    "* Load the image using pylab.imread().\n",
    "* Find and use the 2-D FFT function in scipy.fftpack, and plot the spectrum (Fourier transform of) the image. Do you have any trouble visualising the spectrum? If so, why?\n",
    "* The spectrum consists of high and low frequency components. The noise is contained in the high-frequency part of the spectrum, so set some of those components to zero (use array slicing).\n",
    "* Apply the inverse Fourier transform to see the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f651a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a55591ad",
   "metadata": {},
   "source": [
    "10.1\\. **Radioactive decay chain** (30 MINUTES)\n",
    "\n",
    "${\\rm Tl}^{208}$ decays to ${\\rm Pb}^{208}$ with a half-lieve of 3.052 minutes. Suppose to start with a sample of 1000 Thallium atoms and 0 of Lead atoms.\n",
    "\n",
    "* Take steps in time of 1 second and at each time-step decide whether each Tl atom has decayed or not, accordingly to the probability $p(t)=1-2^{-t/\\tau}$. Subtract the total number of Tl atoms that decayed at each step from the Tl sample and add them to the Lead one. Plot the evolution of the two sets as a function of time  \n",
    "* Repeat the exercise by means of the inverse transform method: draw 1000 random numbers from the non-uniform probability distribution $p(t)=2^{-t/\\tau}\\frac{\\ln 2}{\\tau}$ to represent the times of decay of the 1000 Tl atoms. Make a plot showing the number of atoms that have not decayed as a function of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0082a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3c4ede5",
   "metadata": {},
   "source": [
    "10.2\\. **Rutherford Scattering** (30 MINUTES)\n",
    "\n",
    "The scattering angle $\\theta$ of $\\alpha$ particles hitting a positively charged nucleus of a Gold atom ($Z=79$) follows the rule:\n",
    "\n",
    "$$\n",
    "\\tan{\\frac{1}{2} \\theta} = \\frac{Z e^2} {2\\pi \\epsilon_0 E b}\n",
    "$$\n",
    "\n",
    "where $E=7.7$ MeV and $b$ beam is the impact parameter. The beam is represented by a 2D gaussian distribution with $\\sigma=a_0/100$ for both coordinates ($a_0$ being the Bohr radius). Assume 1 million $\\alpha$ particles are shot on the gold atom.\n",
    "\n",
    "Computing the fraction of particles that \"bounce back\",i.e. those particle whose scattering angle is greater than $\\pi/2$ (which set a condition on the impact parameter $b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b0136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3abc9fd2",
   "metadata": {},
   "source": [
    "10.3\\. **Monte Carlo integration: hit/miss vs mean value method** (30 MINUTES)\n",
    "\n",
    "Consider the function \n",
    "\n",
    "$$f(x) =\\sin^2{\\frac{1}{x(2-x)}}$$\n",
    "\n",
    "* Compute the integral of $f(x)$ between 0 and 2 with the hit/miss method. Evaluate the error of your estimate\n",
    "* Repeat the integral with the mean value method. Evaluate the error and compare it with the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a63df3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1664d19c",
   "metadata": {},
   "source": [
    "10.4\\. **Monte Carlo integration in high dimension** (30 MINUTES)\n",
    "\n",
    "* Start of by computing the area of a circle of unit radius, by integrating the function \n",
    "\n",
    "$$\n",
    "f(x,y)=\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & x^2+y^2\\le 1 \\\\\n",
    "      0 & {\\rm elsewhere}\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "* Generalize the result for a 10D sphere\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ae0fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74850d3f",
   "metadata": {},
   "source": [
    "10.5\\. **Monte Carlo integration with importance sampling**  (30 MINUTES)\n",
    "\n",
    "Calculate the value of the integral:\n",
    "\n",
    "$$\n",
    "I=\\int_0^1 \\frac{x^{-1/2}}{e^x+1} dx\n",
    "$$\n",
    "\n",
    "using the importance sampling method with $w(x)=1/\\sqrt{x}$. You should get a result about 0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fc3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
